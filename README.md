# Logistic Regression and Perceptron Trick - Complete Understanding

---

## ðŸ“š Introduction

This project is focused on implementing and understanding two fundamental machine learning algorithms:

- **Perceptron Learning Algorithm** (manually)
- **Logistic Regression** (using Scikit-learn)

We are working with **linearly separable** data to predict **binary outcomes** (two classes).

Both approaches are visualized with decision boundaries for better intuition.

---

## ðŸ§  Why Use Logistic Regression?

Logistic Regression is one of the most widely used classification techniques because:

1. **Simplicity and Interpretability**  
   It models the probability that a given input belongs to a particular class.

2. **Efficiency in Training and Prediction**  
   Logistic Regression is fast and lightweight.

3. **Probability-Based Output**  
   It doesn't just predict classes but gives **probabilities**.

4. **Works Well for Linearly Separable Data**  
   When two classes can be separated with a straight line or hyperplane, Logistic Regression performs well.

5. **Handles High-dimensional Data**  
   It scales better for moderately large feature spaces.

---

## âš¡ Key Limitation

- **Not suitable for non-linear decision boundaries.**  
  If the classes are not linearly separable, Logistic Regression might perform poorly unless feature engineering is applied (like polynomial features).

---

## ðŸ“ˆ Data Used

We used **synthetic data** generated by `make_classification` from scikit-learn with these properties:
- **Two features** (for 2D plotting)
- **Linearly separable clusters** (no redundancy)

```python
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=2, n_informative=1, 
                            n_redundant=0, random_state=41, n_clusters_per_class=1)
